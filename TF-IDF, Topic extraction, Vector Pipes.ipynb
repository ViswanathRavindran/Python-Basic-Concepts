{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parts of code snippets are from:\n",
    "Illustrates the TFIDF from mathemaics to scikits learn\n",
    "\n",
    "a. http://blog.christianperone.com/2011/09/machine-learning-text-feature-extraction-tf-idf-part-i/\n",
    "\n",
    "b. http://blog.christianperone.com/2011/10/machine-learning-text-feature-extraction-tf-idf-part-ii/\n",
    "\n",
    "Another approach to understand TFIDF\n",
    "\n",
    "c. https://www.youtube.com/watch?v=hXNbFNCgPfY\n",
    "\n",
    "d. https://www.youtube.com/watch?v=BJ0MnawUpaU&t=352s\n",
    "\n",
    "Illustrates some fucntion that can be used on the TFIDF\n",
    "\n",
    "e. https://buhrmann.github.io/tfidf-analysis.html\n",
    "\n",
    "TF-IDF stands for \"Term Frequency, Inverse Document Frequency\". It is a way to score the importance of words (or \"terms\") in a document based on how frequently they appear across multiple documents.\n",
    "1. If a word appears frequently in a document, it's important. Give the word a high score.\n",
    "2. But if a word appears in many documents, it's not a unique identifier. Give the word a low score.\n",
    "\n",
    "Therefore, common words like \"the\" and \"for\", which appear in many documents, will be scaled down. Words that appear frequently in a single document will be scaled up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector Space Model is a algebraic model of converting textual information as a vector and it represents the features extracted from the document. Step 1 is to create a dictionary of all terms in the document into dimensions ignoring the common english terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set = (\"The sky is blue.\", \"The sun is bright.\")\n",
    "test_set = (\"The sun in the sky is bright.\",\n",
    "    \"We can see the shining sun, the bright sun.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scikit.learn, what we have presented as the term-frequency, \n",
    "# is called CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n"
     ]
    }
   ],
   "source": [
    "print (vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 5, 'sky': 3, 'is': 2, 'blue': 0, 'sun': 4, 'bright': 1}\n"
     ]
    }
   ],
   "source": [
    "# Here is the vocabulary index without using the stop words \n",
    "vectorizer.fit_transform(train_set)\n",
    "print (vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 5)\t2\n",
      "  (1, 1)\t1\n",
      "  (1, 4)\t2\n",
      "  (1, 5)\t2\n"
     ]
    }
   ],
   "source": [
    "# We then use the same vectorizer to create a sparse matrix. It is a \n",
    "# Scipy sparse matrix with elements stored in a Coordinate format.\n",
    "# the spare matrix will be represented as \n",
    "# (Document#, dictionary_word) Number of occurences\n",
    "smatrix = vectorizer.transform(test_set)\n",
    "print (smatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 1 2]\n",
      " [0 1 0 0 2 2]]\n"
     ]
    }
   ],
   "source": [
    "# We can convert this sparse matrix into a dense matrix\n",
    "# the dense matrix will have the shape number of documents * no of words\n",
    "# Every element is represented as number of occurences\n",
    "# Each row is count of (blue, bright, is, sky, sun, the)\n",
    "dmatrix = smatrix.todense()\n",
    "print (dmatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDF: [ 2.09861229  1.          1.40546511  1.40546511  1.          1.        ]\n"
     ]
    }
   ],
   "source": [
    "# The tf-idf comes to our rescue for the problem observed above\n",
    "# tf-idf then does to solve that problem, is to scale down the frequent \n",
    "# terms while scaling up the rare terms; a term that occurs 10 times \n",
    "# more than another isn’t 10 times more important than it, that’s why \n",
    "# tf-idf uses the logarithmic scale to do that\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer(norm=\"l2\")\n",
    "tfidf.fit(smatrix)\n",
    "print (\"IDF:\", tfidf.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.31701073  0.44554752  0.44554752  0.31701073  0.63402146]\n",
      " [ 0.          0.33333333  0.          0.          0.66666667  0.66666667]]\n"
     ]
    }
   ],
   "source": [
    "tf_idf_matrix = tfidf.transform(smatrix)\n",
    "print (tf_idf_matrix.todense())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Trying this with another example. In this example we will be performing the TFIDF without using scikits learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Understading the tfidf in much more detail\n",
    "docA = 'the cat sat on my face'\n",
    "docB = 'the dog sat on my bed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tokening \n",
    "bowA = docA.split(\" \")\n",
    "bowB = docB.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'cat', 'sat', 'on', 'my', 'face']\n"
     ]
    }
   ],
   "source": [
    "print (bowA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let us create a set of all words\n",
    "wordSet = set(bowA).union(set(bowB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the', 'sat', 'my', 'face', 'bed', 'cat', 'on', 'dog'}\n"
     ]
    }
   ],
   "source": [
    "print (wordSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create dictionaries for each word count\n",
    "wordDictA = dict.fromkeys(wordSet, 0)\n",
    "wordDictB = dict.fromkeys(wordSet, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bed': 0, 'cat': 0, 'dog': 0, 'face': 0, 'my': 0, 'on': 0, 'sat': 0, 'the': 0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordDictA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Count of the number of word and set in the dict\n",
    "for word in bowA:\n",
    "    wordDictA[word]+=1\n",
    "\n",
    "for word in bowB:\n",
    "    wordDictB[word]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bed': 0, 'cat': 1, 'dog': 0, 'face': 1, 'my': 1, 'on': 1, 'sat': 1, 'the': 1}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordDictA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bed</th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "      <th>face</th>\n",
       "      <th>my</th>\n",
       "      <th>on</th>\n",
       "      <th>sat</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bed  cat  dog  face  my  on  sat  the\n",
       "0    0    1    0     1   1   1    1    1\n",
       "1    1    0    1     0   1   1    1    1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# put the dictionary into a df\n",
    "import pandas as pd\n",
    "pd.DataFrame([wordDictA, wordDictB])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the matrix above has the common words usually available, this is the problem tfidf can solve\n",
    "# TF_IDF = tf(w)*idf(w)\n",
    "# tf(w) = (Number of times the word appears in a document) / Total number of word in the document\n",
    "# idf(w) = log(Number of documents/ Number of documents that contain word w)\n",
    "def computeTF(wordDict, bow):\n",
    "    tfDict = {}\n",
    "    bowCount = len(bow)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count/float(bowCount)\n",
    "    return tfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfBowA = computeTF(wordDictA, bowA)\n",
    "tfBowB = computeTF(wordDictB, bowB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 0.16666666666666666, 'sat': 0.16666666666666666, 'my': 0.16666666666666666, 'face': 0.16666666666666666, 'bed': 0.0, 'cat': 0.16666666666666666, 'on': 0.16666666666666666, 'dog': 0.0}\n"
     ]
    }
   ],
   "source": [
    "print (tfBowA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeIDF(docList):\n",
    "    import math\n",
    "    idfDict = {}\n",
    "    N = len(docList)\n",
    "    \n",
    "    # count the documents that contains word w\n",
    "    idfDict = dict.fromkeys(docList[0].keys(), 0)\n",
    "    for doc in docList:\n",
    "        for word, val in doc.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "    # divide N by denominator above and take a log\n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log(N/ float(val))\n",
    "    return idfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idfs =  computeIDF([wordDictA, wordDictB])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 0.0, 'sat': 0.0, 'my': 0.0, 'face': 0.6931471805599453, 'bed': 0.6931471805599453, 'cat': 0.6931471805599453, 'on': 0.0, 'dog': 0.6931471805599453}\n"
     ]
    }
   ],
   "source": [
    "print (idfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeTFIDF(tfBow, Idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBow.items():\n",
    "        tfidf[word] = val * idfs[word]\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidfBowA = computeTFIDF(tfBowA, idfs)\n",
    "tfidfBowB = computeTFIDF(tfBowB, idfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bed</th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "      <th>face</th>\n",
       "      <th>my</th>\n",
       "      <th>on</th>\n",
       "      <th>sat</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.115525</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.115525</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.115525</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.115525</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        bed       cat       dog      face   my   on  sat  the\n",
       "0  0.000000  0.115525  0.000000  0.115525  0.0  0.0  0.0  0.0\n",
       "1  0.115525  0.000000  0.115525  0.000000  0.0  0.0  0.0  0.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Putting this into a Matrix\n",
    "pd.DataFrame([tfidfBowA, tfidfBowB])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us apply the TFIDF from sklearn and compare the results\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(analyzer='word',lowercase='True', stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'cat', 'sat', 'on', 'my', 'face']\n"
     ]
    }
   ],
   "source": [
    "print(bowA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.006s.\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "t0 = time()\n",
    "tfidf_vectorizer.fit(wordSet)\n",
    "#tfidf_vectorizer.\n",
    "tfidf_A = tfidf_vectorizer.fit_transform(bowA)\n",
    "tfidf_B = tfidf_vectorizer.fit_transform(bowB)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (1, 0)\t1.0\n",
      "  (2, 2)\t1.0\n",
      "  (5, 1)\t1.0\n"
     ]
    }
   ],
   "source": [
    "print (tfidf_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fin_tfidfA = tfidf_A.todense()\n",
    "fin_tfidfB = tfidf_B.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print (fin_tfidfA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 3)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fin_tfidfA.shape\n",
    "fin_tfidfA.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bed', 'dog', 'sat']\n"
     ]
    }
   ],
   "source": [
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print (feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using TFIDF to perform topic extraction\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "done in 2.721s.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading dataset...\")\n",
    "t0 = time()\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1,\n",
    "                             remove=('headers', 'footers', 'quotes'))\n",
    "data_samples = dataset.data[:n_samples]\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "print (len(data_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'off', 'here', 'hasn', 'over', 'against', 'their', 'but', 'above', 'needn', 'doesn', 'this', 'down', 'i', 'how', 'ourselves', 'too', 'with', 'no', 'any', 'hers', 'just', 'own', 'himself', 'did', 'couldn', 'won', 'yourselves', 'nor', 'herself', 'so', 'to', 'during', 'up', 'than', 'mustn', 'each', 'where', 'all', 'into', 'only', 's', 't', 'having', 'if', 'what', 'from', 'before', 'after', 'are', 'ours', 'through', 'me', 'between', 'can', 'is', 'more', 'an', 'it', 'as', 'o', 'very', 'was', 'had', 'its', 'mightn', 'should', 'out', 've', 'which', 'ain', 'whom', 'both', 'then', 'd', 'didn', 'you', 'were', 'same', 'itself', 'a', 'haven', 'most', 'the', 'of', 'weren', 'yourself', 'below', 'further', 'his', 'we', 'there', 'll', 'such', 'on', 'these', 'at', 'isn', 'when', 'other', 'myself', 'am', 'until', 'now', 'not', 'by', 'that', 'or', 'under', 'my', 'does', 'yours', 'for', 'who', 'be', 'him', 're', 'they', 'few', 'wouldn', 'been', 'shan', 'why', 'will', 'being', 'in', 'she', 'them', 'again', 'has', 'while', 'about', 'don', 'your', 'do', 'y', 'hadn', 'some', 'm', 'our', 'he', 'wasn', 'doing', 'ma', 'those', 'themselves', 'and', 'have', 'once', 'her', 'because', 'aren', 'shouldn', 'theirs'}\n"
     ]
    }
   ],
   "source": [
    "stopset = set(stopwords.words('english'))\n",
    "print (stopset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Well i'm not sure about the story nad it did seem biased. What\\nI disagree with is your statement that the U.S. Media is out to\\nruin Israels reputation. That is rediculous. The U.S. media is\\nthe most pro-israeli media in the world. Having lived in Europe\\nI realize that incidences such as the one described in the\\nletter have occured. The U.S. media as a whole seem to try to\\nignore them. The U.S. is subsidizing Israels existance and the\\nEuropeans are not (at least not to the same degree). So I think\\nthat might be a reason they report more clearly on the\\natrocities.\\n\\tWhat is a shame is that in Austria, daily reports of\\nthe inhuman acts commited by Israeli soldiers and the blessing\\nreceived from the Government makes some of the Holocaust guilt\\ngo away. After all, look how the Jews are treating other races\\nwhen they got power. It is unfortunate.\\n\""
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let us start with the first step of vectorizing\n",
    "data_samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=stopset, use_idf=True, ngram_range=(1,3), max_features=5000, analyzer='word', \n",
    "                             lowercase=True, norm='l2')\n",
    "X = vectorizer.fit_transform(data_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x5000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 46 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4844)\t0.0711724269128\n",
      "  (0, 4370)\t0.0849337242543\n",
      "  (0, 4294)\t0.119602186967\n",
      "  (0, 3962)\t0.196050256492\n",
      "  (0, 695)\t0.154891800464\n",
      "  (0, 1413)\t0.136647103602\n",
      "  (0, 4260)\t0.117365134839\n",
      "  (0, 2789)\t0.48825320374\n",
      "  (0, 3491)\t0.134410051474\n",
      "  (0, 2369)\t0.283686649061\n",
      "  (0, 4916)\t0.0963772935546\n",
      "  (0, 2631)\t0.134410051474\n",
      "  (0, 1634)\t0.14184332453\n",
      "  (0, 3644)\t0.129572824027\n",
      "  (0, 3142)\t0.0557308970244\n",
      "  (0, 1346)\t0.134410051474\n",
      "  (0, 2570)\t0.127876246769\n",
      "  (0, 3114)\t0.15748268052\n",
      "  (0, 4860)\t0.105344092594\n",
      "  (0, 4615)\t0.0905284227811\n",
      "  (0, 2211)\t0.135503147729\n",
      "  (0, 1669)\t0.15748268052\n",
      "  (0, 1636)\t0.167453317579\n",
      "  (0, 2541)\t0.0931792069269\n",
      "  (0, 1319)\t0.144921163404\n",
      "  (0, 4500)\t0.0679603960735\n",
      "  (0, 2835)\t0.0849337242543\n",
      "  (0, 3651)\t0.102851772256\n",
      "  (0, 3736)\t0.124085586487\n",
      "  (0, 995)\t0.119602186967\n",
      "  (0, 4029)\t0.154891800464\n",
      "  (0, 1257)\t0.144921163404\n",
      "  (0, 3738)\t0.129572824027\n",
      "  (0, 340)\t0.13910821757\n",
      "  (0, 4141)\t0.140437763884\n",
      "  (0, 3662)\t0.116318567674\n",
      "  (0, 1994)\t0.0970700327922\n",
      "  (0, 2715)\t0.102851772256\n",
      "  (0, 2151)\t0.154891800464\n",
      "  (0, 1965)\t0.0824862301246\n",
      "  (0, 599)\t0.102851772256\n",
      "  (0, 2650)\t0.0898207302947\n",
      "  (0, 2398)\t0.123392847249\n",
      "  (0, 1990)\t0.0881357825555\n",
      "  (0, 3435)\t0.0954829907564\n",
      "  (0, 2370)\t0.163662657297\n"
     ]
    }
   ],
   "source": [
    "print (X[0])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# LSA - Latenet Semantic Analysis\n",
    "The input is X a matrix where m si the number of documents we have and n is the number of terms\n",
    "We will then decompose X into three matrices called U, S, V. While decomposing we have to select a value k which indicates the number of concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 5000)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=27, n_iter=100,\n",
       "       random_state=None, tol=0.0)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa = TruncatedSVD(n_components=27, n_iter=100)\n",
    "lsa.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.02491891,  0.02282795,  0.00282885, ...,  0.00723936,\n",
       "        0.00481203,  0.00079901])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa.components_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concept 0:\n",
      "would\n",
      "one\n",
      "like\n",
      "people\n",
      "know\n",
      "get\n",
      "think\n",
      "good\n",
      "time\n",
      "could\n",
      " \n",
      "Concept 1:\n",
      "god\n",
      "people\n",
      "think\n",
      "jesus\n",
      "bible\n",
      "even\n",
      "us\n",
      "say\n",
      "law\n",
      "government\n",
      " \n",
      "Concept 2:\n",
      "geb\n",
      "gordon banks\n",
      "pitt edu\n",
      "cadre\n",
      "banks n3jxp\n",
      "banks n3jxp skepticism\n",
      "cadre dsl\n",
      "cadre dsl pitt\n",
      "chastity\n",
      "chastity intellect\n",
      " \n",
      "Concept 3:\n",
      "god\n",
      "thanks\n",
      "please\n",
      "bible\n",
      "jesus\n",
      "anyone\n",
      "know\n",
      "mail\n",
      "advance\n",
      "christian\n",
      " \n",
      "Concept 4:\n",
      "thanks\n",
      "would\n",
      "anyone\n",
      "please\n",
      "advance\n",
      "know\n",
      "game\n",
      "thanks advance\n",
      "mail\n",
      "could\n",
      " \n",
      "Concept 5:\n",
      "key\n",
      "chip\n",
      "government\n",
      "clipper\n",
      "encryption\n",
      "keys\n",
      "use\n",
      "law\n",
      "clipper chip\n",
      "system\n",
      " \n",
      "Concept 6:\n",
      "drive\n",
      "god\n",
      "00\n",
      "chip\n",
      "car\n",
      "new\n",
      "sale\n",
      "drives\n",
      "please\n",
      "hard\n",
      " \n",
      "Concept 7:\n",
      "edu\n",
      "com\n",
      "mail\n",
      "please\n",
      "space\n",
      "send\n",
      "ftp\n",
      "00\n",
      "last\n",
      "information\n",
      " \n",
      "Concept 8:\n",
      "game\n",
      "chip\n",
      "key\n",
      "team\n",
      "god\n",
      "games\n",
      "clipper\n",
      "keys\n",
      "play\n",
      "encryption\n",
      " \n",
      "Concept 9:\n",
      "would\n",
      "drive\n",
      "edu\n",
      "israel\n",
      "drives\n",
      "00\n",
      "software\n",
      "com\n",
      "computer\n",
      "people\n",
      " \n",
      "Concept 10:\n",
      "would\n",
      "god\n",
      "car\n",
      "like\n",
      "00\n",
      "key\n",
      "would like\n",
      "edu\n",
      "use\n",
      "windows\n",
      " \n",
      "Concept 11:\n",
      "edu\n",
      "com\n",
      "file\n",
      "drive\n",
      "get\n",
      "ftp\n",
      "problem\n",
      "try\n",
      "car\n",
      "think\n",
      " \n",
      "Concept 12:\n",
      "space\n",
      "like\n",
      "nasa\n",
      "data\n",
      "time\n",
      "window\n",
      "earth\n",
      "computer\n",
      "moon\n",
      "launch\n",
      " \n",
      "Concept 13:\n",
      "card\n",
      "one\n",
      "israel\n",
      "window\n",
      "video\n",
      "pc\n",
      "edu\n",
      "get\n",
      "looking\n",
      "right\n",
      " \n",
      "Concept 14:\n",
      "israel\n",
      "please\n",
      "drive\n",
      "could\n",
      "window\n",
      "key\n",
      "israeli\n",
      "bike\n",
      "advance\n",
      "also\n",
      " \n",
      "Concept 15:\n",
      "file\n",
      "israel\n",
      "00\n",
      "car\n",
      "anyone\n",
      "know\n",
      "card\n",
      "god\n",
      "key\n",
      "anyone know\n",
      " \n",
      "Concept 16:\n",
      "anyone\n",
      "looking\n",
      "car\n",
      "information\n",
      "book\n",
      "good\n",
      "software\n",
      "would\n",
      "problem\n",
      "help\n",
      " \n",
      "Concept 17:\n",
      "get\n",
      "file\n",
      "card\n",
      "could\n",
      "car\n",
      "space\n",
      "someone\n",
      "drivers\n",
      "year\n",
      "think\n",
      " \n",
      "Concept 18:\n",
      "com\n",
      "thanks\n",
      "know\n",
      "god\n",
      "thanks advance\n",
      "advance\n",
      "windows\n",
      "power\n",
      "system\n",
      "right\n",
      " \n",
      "Concept 19:\n",
      "space\n",
      "looking\n",
      "help\n",
      "would\n",
      "please\n",
      "people\n",
      "appreciated\n",
      "us\n",
      "know\n",
      "mail\n",
      " \n",
      "Concept 20:\n",
      "com\n",
      "car\n",
      "please\n",
      "speed\n",
      "memory\n",
      "time\n",
      "chip\n",
      "windows\n",
      "people\n",
      "power\n",
      " \n",
      "Concept 21:\n",
      "bike\n",
      "file\n",
      "com\n",
      "one\n",
      "good\n",
      "chip\n",
      "clipper\n",
      "thanks\n",
      "right\n",
      "dog\n",
      " \n",
      "Concept 22:\n",
      "com\n",
      "00\n",
      "card\n",
      "stuff\n",
      "really\n",
      "looking\n",
      "one\n",
      "space\n",
      "sale\n",
      "get\n",
      " \n",
      "Concept 23:\n",
      "window\n",
      "someone\n",
      "could\n",
      "new\n",
      "00\n",
      "looking\n",
      "windows\n",
      "try\n",
      "software\n",
      "back\n",
      " \n",
      "Concept 24:\n",
      "chip\n",
      "problem\n",
      "think\n",
      "clipper\n",
      "israel\n",
      "could\n",
      "bike\n",
      "clipper chip\n",
      "00\n",
      "someone\n",
      " \n",
      "Concept 25:\n",
      "bike\n",
      "windows\n",
      "israel\n",
      "software\n",
      "dos\n",
      "year\n",
      "drivers\n",
      "ftp\n",
      "anyone\n",
      "version\n",
      " \n",
      "Concept 26:\n",
      "use\n",
      "israel\n",
      "com\n",
      "thanks advance\n",
      "good\n",
      "thanks\n",
      "advance\n",
      "ftp\n",
      "also\n",
      "anybody\n",
      " \n"
     ]
    }
   ],
   "source": [
    "terms = vectorizer.get_feature_names()\n",
    "for i, comp in enumerate(lsa.components_):\n",
    "    termsInComp = zip(terms,comp)\n",
    "    sortedTerms = sorted(termsInComp, key=lambda x: x[1], reverse=True) [:10]\n",
    "    print (\"Concept %d:\" %i)\n",
    "    for term in sortedTerms:\n",
    "        print (term[0])\n",
    "    print (\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m11\u001b[0m\n\u001b[0;31m    vec_pipe = [\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "# Creting a vec pipe\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "''' Create text vectorization pipeline with optional dimensionality reduction. '''\n",
    "def get_vec_pipe(num_comp=0, reducer='svd'):\n",
    "    tfv = TfidfVectorizer(min_df=6, max_features=None, strip_accents='unicode',analyzer=\"word\", \n",
    "                          token_pattern=r'\\w{1,}', ngram_range=(1, 2),use_idf=1, smooth_idf=1, sublinear_tf=1)\n",
    "    # Vectorizer\n",
    "    vec_pipe = [\n",
    "        ('col_extr', JsonFields(0, ['title', 'body', 'url'])),\n",
    "        ('squash', Squash()),\n",
    "        ('vec', tfv)\n",
    "    ]\n",
    "\n",
    "    # Reduce dimensions of tfidf\n",
    "    if num_comp > 0:\n",
    "        if reducer == 'svd':\n",
    "            vec_pipe.append(('dim_red', TruncatedSVD(num_comp)))\n",
    "        elif reducer == 'kbest':\n",
    "            vec_pipe.append(('dim_red', SelectKBest(chi2, k=num_comp)))\n",
    "        elif reducer == 'percentile':\n",
    "            vec_pipe.append(('dim_red', SelectPercentile(f_classif, percentile=num_comp)))\n",
    "\n",
    "        vec_pipe.append(('norm', Normalizer()))\n",
    "\n",
    "    return Pipeline(vec_pipe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def top_tfidf_feats(row, features, top_n=10):\n",
    "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
    "    top_feats = [(features[i], row[i]) for i in topn_ids]\n",
    "    df = pd.DataFrame(top_feats)\n",
    "    df.columns = ['feature', 'tfidf']\n",
    "    return df\n",
    "\n",
    "def top_feats_in_doc(Xtr, features, row_id, top_n=10):\n",
    "    row = np.squeeze(Xtr[row_id].toarray())\n",
    "    return top_tfidf_feats(row, features, top_n)\n",
    "\n",
    "def top_mean_feats(Xtr, features, grp_ids=None, min_tfidf=0.1, top_n=10):\n",
    "    if grp_ids:\n",
    "        D = Xtr[grp_ids].toarray()\n",
    "    else:\n",
    "        D = Xtr.toarray()\n",
    "\n",
    "    D[D < min_tfidf] = 0\n",
    "    tfidf_means = np.mean(D, axis=0)\n",
    "    return top_tfidf_feats(tfidf_means, features, top_n)\n",
    "\n",
    "def top_feats_by_class(Xtr, y, features, min_tfidf=0.1, top_n=10):\n",
    "    dfs = []\n",
    "    labels = np.unique(y)\n",
    "    for label in labels:\n",
    "        ids = np.where(y==label)\n",
    "        feats_df = top_mean_feats(Xtr, features, ids, min_tfidf=min_tfidf, top_n=top_n)\n",
    "        feats_df.label = label\n",
    "        dfs.append(feats_df)\n",
    "    return dfs\n",
    "\n",
    "def plot_tfidf_classfeats_h(dfs):\n",
    "    fig = plt.figure(figsize=(12, 100), facecolor=\"w\")\n",
    "    x = np.arange(len(dfs[0]))\n",
    "    for i, df in enumerate(dfs):\n",
    "        #z = int(str(int(i/3)+1) + str((i%3)+1))\n",
    "        ax = fig.add_subplot(9, 1, i+1)\n",
    "        ax.spines[\"top\"].set_visible(False)\n",
    "        ax.spines[\"right\"].set_visible(False)\n",
    "        ax.set_frame_on(False)\n",
    "        ax.get_xaxis().tick_bottom()\n",
    "        ax.get_yaxis().tick_left()\n",
    "        ax.set_xlabel(\"Mean Tf-Idf Score\", labelpad=16, fontsize=16)\n",
    "        ax.set_ylabel(\"Gene\", labelpad=16, fontsize=16)\n",
    "        ax.set_title(\"Class = \" + str(df.label), fontsize=18)\n",
    "        ax.ticklabel_format(axis='x', style='sci', scilimits=(-2,2))\n",
    "        ax.barh(x, df.tfidf, align='center')\n",
    "        ax.set_yticks(x)\n",
    "        ax.set_ylim([-1, x[-1]+1])\n",
    "        yticks = ax.set_yticklabels(df.feature)\n",
    "        plt.subplots_adjust(bottom=0.09, right=0.97, left=0.15, top=0.95, wspace=0.52)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
