{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using TFIDF to perform topic extraction\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from time import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myfile = 'C:/Users/u220506/Documents/Projects/Topic Modelling/vegastext.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "file = open('C:/Users/u220506/Documents/Projects/Topic Modelling/vegastext.txt', 'r')\n",
    "text = file.read()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "5991\n",
      "M\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print (type(text))\n",
    "print (len(text))\n",
    "print (text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data = list(zip(*list_of_lists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "EXAMPLE_TEXT = text\n",
    "#print(sent_tokenize(EXAMPLE_TEXT))\n",
    "#print(word_tokenize(EXAMPLE_TEXT))\n",
    "word_tokens = word_tokenize(EXAMPLE_TEXT)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "filtered_sentence = []\n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "\n",
    "#print(word_tokens)\n",
    "#print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#del list_of_lists\n",
    "#stopset = set(stopwords.words('english'))\n",
    "vectorizer = TfidfVectorizer(stop_words=stopset, use_idf=True, ngram_range=(1,2), max_features=5000, analyzer='word', \n",
    "                             lowercase=True)\n",
    "X = vectorizer.fit_transform(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "  (0, 240)\t1.0\n",
      "(713, 399)\n"
     ]
    }
   ],
   "source": [
    "print (type(X))\n",
    "print (X[0])\n",
    "print (X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='arpack', n_components=27, n_iter=100,\n",
       "       random_state=None, tol=0.0)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa = TruncatedSVD(n_components=27, algorithm='arpack', n_iter=100)\n",
    "lsa.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.decomposition.truncated_svd.TruncatedSVD'>\n"
     ]
    }
   ],
   "source": [
    "print (type(lsa))\n",
    "#print (lsa.components_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concept 0:\n",
      "shooting\n",
      "october\n",
      "hours\n",
      "victims\n",
      "trump\n",
      " \n",
      "Concept 1:\n",
      "october\n",
      "false\n",
      "international\n",
      "group\n",
      "officials\n",
      " \n",
      "Concept 2:\n",
      "false\n",
      "hotel\n",
      "trump\n",
      "vegas\n",
      "officials\n",
      " \n",
      "Concept 3:\n",
      "false\n",
      "stories\n",
      "news\n",
      "october\n",
      "official\n",
      " \n",
      "Concept 4:\n",
      "police\n",
      "news\n",
      "false\n",
      "investigation\n",
      "4chan\n",
      " \n",
      "Concept 5:\n",
      "news\n",
      "october\n",
      "control\n",
      "website\n",
      "failed\n",
      " \n",
      "Concept 6:\n",
      "paddock\n",
      "statement\n",
      "shooter\n",
      "information\n",
      "victims\n",
      " \n",
      "Concept 7:\n",
      "shooter\n",
      "october\n",
      "victims\n",
      "statement\n",
      "hotel\n",
      " \n",
      "Concept 8:\n",
      "shooter\n",
      "campos\n",
      "trump\n",
      "man\n",
      "las\n",
      " \n",
      "Concept 9:\n",
      "man\n",
      "vegas\n",
      "investigation\n",
      "al baghdadi\n",
      "police\n",
      " \n",
      "Concept 10:\n",
      "said\n",
      "officials\n",
      "campos\n",
      "man\n",
      "shooter\n",
      " \n",
      "Concept 11:\n",
      "said\n",
      "las\n",
      "vegas\n",
      "trump\n",
      "shooter\n",
      " \n",
      "Concept 12:\n",
      "trump\n",
      "said\n",
      "officials\n",
      "al\n",
      "dollars\n",
      " \n",
      "Concept 13:\n",
      "vegas\n",
      "campos\n",
      "said\n",
      "shooter\n",
      "fire\n",
      " \n",
      "Concept 14:\n",
      "shooter\n",
      "officials\n",
      "vegas\n",
      "trump\n",
      "man\n",
      " \n",
      "Concept 15:\n",
      "gun\n",
      "control\n",
      "gun control\n",
      "paddock\n",
      "statement\n",
      " \n",
      "Concept 16:\n",
      "facebook\n",
      "hotel\n",
      "terrorist\n",
      "site\n",
      "search\n",
      " \n",
      "Concept 17:\n",
      "fbi\n",
      "two\n",
      "site\n",
      "international\n",
      "hotel\n",
      " \n",
      "Concept 18:\n",
      "fire\n",
      "official\n",
      "bump\n",
      "hotel\n",
      "international\n",
      " \n",
      "Concept 19:\n",
      "facebook\n",
      "fbi\n",
      "bump\n",
      "group\n",
      "also\n",
      " \n",
      "Concept 20:\n",
      "information\n",
      "statement\n",
      "victims\n",
      "international\n",
      "group\n",
      " \n",
      "Concept 21:\n",
      "official\n",
      "two\n",
      "international\n",
      "hours\n",
      "terrorist\n",
      " \n",
      "Concept 22:\n",
      "hotel\n",
      "hours\n",
      "fbi\n",
      "statement\n",
      "international\n",
      " \n",
      "Concept 23:\n",
      "international\n",
      "group\n",
      "site\n",
      "fire\n",
      "hours\n",
      " \n",
      "Concept 24:\n",
      "terrorist\n",
      "hotel\n",
      "group\n",
      "bump\n",
      "facebook\n",
      " \n",
      "Concept 25:\n",
      "information\n",
      "international\n",
      "isis\n",
      "two\n",
      "fbi\n",
      " \n",
      "Concept 26:\n",
      "also\n",
      "group\n",
      "hotel\n",
      "isis\n",
      "facebook\n",
      " \n"
     ]
    }
   ],
   "source": [
    "terms = vectorizer.get_feature_names()\n",
    "for i, comp in enumerate(lsa.components_):\n",
    "    termsInComp = zip(terms,comp)\n",
    "    sortedTerms = sorted(termsInComp, key=lambda x: x[1], reverse=True) [:5]\n",
    "    print (\"Concept %d:\" %i)\n",
    "    for term in sortedTerms:\n",
    "        print (term[0])\n",
    "    print (\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
